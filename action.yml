name: Setup llama.cpp
description: Setup llama.cpp (Windows-only)
branding:
  icon: play
  color: yellow
inputs:
    llama_version:
        description: 'version of llama.cpp'
        required: true
        default: "b6325"
        type: string
    question:
        description: 'question text'
        required: false
        type: string
    question_file:
        description: 'Question text file path'
        required: false
        type: string
    batch-size:
        description: 'llama --batch-size N'
        required: false
        default: 512
        type: number
    n-gpu-layers:
        description: 'llama --n-gpu-layers N'
        required: false
        default: 50
        type: number
    seed:
        description: 'llama --seed N, 0 = random'
        required: false
        default: 12345
        type: number
    threads:
        description: 'llama --threads N, 0 = auto'
        required: false
        default: 0
        type: number
    predict:
        description: 'llama --predict N'
        required: false
        default: 8192
        type: number 
    ctx-size:
        description: 'llama --ctx-size N'
        required: false
        default: 32768
        type: number
    mlock:
        description: 'llama --mlock'
        required: false
        default: false
        type: boolean
    simple-io:
        description: 'llama --simple-io'
        required: false
        default: false
        type: boolean
    model:
        description: '.gguf model to use'
        required: true
        default: 'openai_gpt-oss-20b-MXFP4'
        type: choice
        options:
        - gemma-3-4b-it-q4_0
        - gemma-3-12b-it-q4_0
        - gemma-3-27b-it-q4_0
        - gemma-3-27b-it-qat-Q8_0
        - gemma-3-27b-it-qat-IQ4_NL
        - gemma-3-27b-it-qat-IQ3_XXS
        - gemma-3-27b-it-qat-IQ3_M
        - gemma-3-27b-it-IQ3_XXS
        - gemma-3-27b-it-IQ3_M
        - openai_gpt-oss-20b-Q8_0
        - openai_gpt-oss-20b-IQ4_NL
        - openai_gpt-oss-20b-MXFP4
        - openai_gpt-oss-120b-Q4_K_M
        - openai_gpt-oss-120b-IQ4_NL
        - openai_gpt-oss-120b-UD-Q4_K_XL
        - cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ3_XXS
        - cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ3_M
        - cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ4_NL
        - cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q3_K_M
        - cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_0
        - cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M
        - cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ3_XXS
        - cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ3_M
        - cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ4_NL
        - cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q3_K_M
        - cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q4_0
        - cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q4_K_M
        - Qwen3-30B-A3B-UD-IQ3_XXS
        - Qwen3-30B-A3B-UD-Q3_K_XL
        - Qwen3-30B-A3B-Q3_K_M
        - Qwen3-30B-A3B-Q4_0
        - Qwen3-30B-A3B-Q4_K_M
        - a-m-team_AM-Thinking-v1-IQ3_M
    HUGGINGFACE_TOKEN:
      description: 'Personal access token for accessing Hugging Face'
      type: string
      required: true

outputs:
  answer:
    value: ${{ steps.out.outputs.answer }}
  exe:
    value: ${{ steps.inference.outputs.exe }}

runs:
  using: "composite"
  steps:
      - name: Fail if not Windows
        if: ${{ runner.os != 'Windows' }}
        shell: bash
        run: exit 1

      - name: Check active session id
        shell: pwsh
        run: |
          Add-Type -TypeDefinition @"
          using System;
          using System.Runtime.InteropServices;
          public class SessionHelper {
              [DllImport("kernel32.dll")]
              public static extern uint WTSGetActiveConsoleSessionId();
          }
          "@

          $sessionId = [SessionHelper]::WTSGetActiveConsoleSessionId()
          Write-Host "Active console session ID: $sessionId"

          if ($sessionId -eq 0) {
              Write-Error "No active console session found. Exiting with code 1."
              exit 1
          }

      - name: Check if NVIDIA GPU with >5GB is available
        id: gpu_check
        shell: pwsh
        run: |
            $env:PATH += ";C:\\Program Files\\NVIDIA Corporation\\NVSMI"
            $hasNvidia = $false
            if (Get-Command nvidia-smi -ErrorAction SilentlyContinue) {
            $gpuInfo = nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits
            foreach ($line in $gpuInfo) {
                $fields = $line -split ","
                $name = $fields[0].Trim()
                $memMB = [int]$fields[1].Trim()
                Write-Output "Detected GPU: $name (${memMB} MB)"
                if ($memMB -ge 5120) {
                $hasNvidia = $true
                }
            }
            } else {
            # fallback to CIM (supported in PowerShell Core)
            $videoControllers = Get-CimInstance Win32_VideoController
            foreach ($vc in $videoControllers) {
                if ($vc.Name -match "NVIDIA") {
                $vramMB = [int]($vc.AdapterRAM / 1MB)
                Write-Output "Detected GPU (CIM): $($vc.Name) (${vramMB} MB)"
                if ($vc.AdapterRAM -ge 5368709120) {
                    $hasNvidia = $true
                }
                }
            }
            }
            echo "nvidia_gpu=$hasNvidia" >> $env:GITHUB_OUTPUT

      # nvidia gpu がある場合、プレビルド cuda バイナリをダウンロード
      - name: Check llama binaries already built
        id: llama_bin_check
        shell: pwsh
        run: |
          if ('${{ steps.gpu_check.outputs.nvidia_gpu }}' -eq 'True') {
            if (Test-Path '${{ runner.tool_cache }}\cuda_dll_12.4\lib\x64\nvblas.lib') {
                if (Test-Path '${{ runner.tool_cache }}\llama_cuda.cpp_${{ inputs.llama_version }}\bin\llama.dll') {
                    echo "exists=true" >> $env:GITHUB_OUTPUT
                }
            }
          } else {
            if (Test-Path '${{ runner.tool_cache }}\llama.cpp_${{ inputs.llama_version }}\bin\llama.dll') {
                echo "exists=true" >> $env:GITHUB_OUTPUT
            }
          }

      - name: Download CUDA Runtime DLL ZIP
        if: ${{ steps.llama_bin_check.outputs.exists != 'true' && steps.gpu_check.outputs.nvidia_gpu == 'True' }}
        shell: pwsh
        run: |
          $extract = '${{ runner.tool_cache }}\cuda_dll_12.4'
          $file = 'cuda_cudart-windows-x86_64-12.4.127-archive'
          $zip = "${{ runner.temp }}\${file}.zip"
          Invoke-WebRequest -Uri "https://developer.download.nvidia.com/compute/cuda/redist/cuda_cudart/windows-x86_64/${file}.zip" -OutFile $zip
          Expand-Archive -Path $zip -DestinationPath $extract -Force

          # コピー（構造維持 + 上書きマージ）
          Copy-Item -Path "${{ runner.tool_cache }}\cuda_dll_12.4\${file}\*" -Destination '${{ runner.tool_cache }}\cuda_dll_12.4' -Recurse -Force

          # コピーが完了したら、元を削除
          Remove-Item -Path "${{ runner.tool_cache }}\cuda_dll_12.4\${file}" -Recurse -Force

          # cublas
          $extract = '${{ runner.tool_cache }}\cuda_dll_12.4'
          $file = 'libcublas-windows-x86_64-12.4.5.8-archive'
          $zip = "${{ runner.temp }}\${file}.zip"
          Invoke-WebRequest -Uri "https://developer.download.nvidia.com/compute/cuda/redist/libcublas/windows-x86_64/${file}.zip" -OutFile $zip
          Expand-Archive -Path $zip -DestinationPath $extract -Force

          # コピー（構造維持 + 上書きマージ）
          Copy-Item -Path "${{ runner.tool_cache }}\cuda_dll_12.4\${file}\*" -Destination '${{ runner.tool_cache }}\cuda_dll_12.4' -Recurse -Force

          # コピーが完了したら、元を削除
          Remove-Item -Path "${{ runner.tool_cache }}\cuda_dll_12.4\${file}" -Recurse -Force

      - name: set CUDA_PATH pass
        if: ${{ steps.gpu_check.outputs.nvidia_gpu == 'True' }}
        shell: pwsh
        run: |
          if (-not (Test-Path '${{ runner.tool_cache }}\cuda_dll_12.4\lib\x64')) {
            exit 1
          }
          '${{ runner.tool_cache }}\cuda_dll_12.4\bin' | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          'CUDA_PATH=${{ runner.tool_cache }}\cuda_dll_12.4' | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
          'CUDA_PATH_V12_4=${{ runner.tool_cache }}\cuda_dll_12.4' | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append

      - name: Download llama-cuda
        if: ${{ steps.llama_bin_check.outputs.exists != 'true' && steps.gpu_check.outputs.nvidia_gpu == 'True' }}
        shell: pwsh
        run: |
          $url = "https://github.com/ggml-org/llama.cpp/releases/download/${{ inputs.llama_version }}/llama-${{ inputs.llama_version }}-bin-win-cuda-12.4-x64.zip"
          $dst = Join-Path $env:RUNNER_TOOL_CACHE 'llama_cuda.cpp_${{ inputs.llama_version }}\bin'
          mkdir $dst -Force | Out-Null
          Invoke-WebRequest -Uri $url -OutFile "cuda.zip"
          Expand-Archive -Path "cuda.zip" -DestinationPath $dst -Force

      # cuda が利用できない場合、 msvc が利用できるなら native 最適化でビルド、利用できないなら cpu プレビルドを使う
      - name: Check MSVC installation using vswhere
        if: ${{ steps.llama_bin_check.outputs.exists != 'true' && steps.gpu_check.outputs.nvidia_gpu != 'True' }}
        id: msvc
        shell: pwsh
        run: |
          Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
          $ErrorActionPreference = 'Stop'

          $vswherePath = "${env:ProgramFiles(x86)}\Microsoft Visual Studio\Installer\vswhere.exe"

          if (-not (Test-Path $vswherePath)) {
            Write-Error "vswhere.exe not found. Visual Studio Installer may not be installed."
            echo "exists=false" >> $env:GITHUB_OUTPUT
            exit 0
          }

          # VC++ツールがインストールされた Visual Studio のパスを取得
          $installPath = & "$vswherePath" -latest -products * `
            -requires Microsoft.VisualStudio.Component.VC.Tools.x86.x64 `
            -property installationPath

          if (-not $installPath) {
            Write-Error "MSVC (VC++ Build Tools) is not installed."
            echo "exists=false" >> $env:GITHUB_OUTPUT
            exit 0
          }

          Write-Host "MSVC installation found at: $installPath"
          echo "exists=true" >> $env:GITHUB_OUTPUT

      # llama プレビルドバイナリのダウンロード
      - name: Download llama-cpu
        if: ${{ steps.llama_bin_check.outputs.exists != 'true' && steps.gpu_check.outputs.nvidia_gpu != 'True' && steps.msvc.outputs.exists != 'true' }}
        shell: pwsh
        run: |
          $url = "https://github.com/ggml-org/llama.cpp/releases/download/${{ inputs.llama_version }}/llama-${{ inputs.llama_version }}-bin-win-cpu-x64.zip"
          $dst = Join-Path $env:RUNNER_TOOL_CACHE 'llama.cpp_${{ inputs.llama_version }}\bin'
          mkdir $dst -Force | Out-Null
          Invoke-WebRequest -Uri $url -OutFile "llama-cpu.zip"
          Expand-Archive -Path "llama-cpu.zip" -DestinationPath $dst

      #llama の自前ビルド
      - name: Download & extract CMake (if not exists)
        if: ${{ steps.llama_bin_check.outputs.exists != 'true' && steps.gpu_check.outputs.nvidia_gpu != 'True' && steps.msvc.outputs.exists == 'true' }}
        shell: pwsh
        run: |
          Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
          $cmakePackage = "cmake-3.29.2-windows-x86_64"
          $cmakeDir = "${{ runner.tool_cache }}\${cmakePackage}"
          if (-Not (Test-Path $cmakeDir)) {
            Invoke-WebRequest -Uri "https://github.com/Kitware/CMake/releases/download/v3.29.2/${cmakePackage}.zip" -OutFile "cmake.zip"
            Expand-Archive -Path "cmake.zip" -DestinationPath ${{ runner.tool_cache }}
          } else {
              Write-Host "using cache $cmakeDir" 
          }

      - name: Add CMake to PATH (permanent)
        if: ${{ steps.llama_bin_check.outputs.exists != 'true' && steps.gpu_check.outputs.nvidia_gpu != 'True' && steps.msvc.outputs.exists == 'true' }}
        shell: pwsh
        run: |
          Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
          echo "${{ runner.tool_cache }}\cmake-3.29.2-windows-x86_64\bin" | Out-File -Append -Encoding utf8 $env:GITHUB_PATH

      - name: Check CMake versions
        if: ${{ steps.llama_bin_check.outputs.exists != 'true' && steps.gpu_check.outputs.nvidia_gpu != 'True' && steps.msvc.outputs.exists == 'true' }}
        shell: pwsh
        run: |
          Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
          $ErrorActionPreference = 'Stop'
          cmake --version

      - name: Checkout llama.cpp ref=${{ inputs.llama_version }}
        if: ${{ steps.llama_bin_check.outputs.exists != 'true' && steps.gpu_check.outputs.nvidia_gpu != 'True' && steps.msvc.outputs.exists == 'true' }}
        uses: actions/checkout@v4
        with:
          repository: ggml-org/llama.cpp
          ref: ${{ inputs.llama_version }}
          path: llama.cpp

      - name: Build llama.cpp with MSVC
        if: ${{ steps.llama_bin_check.outputs.exists != 'true' && steps.gpu_check.outputs.nvidia_gpu != 'True' && steps.msvc.outputs.exists == 'true' }}
        shell: pwsh
        run: |
          Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
          cd llama.cpp
          New-Item -ItemType Directory -Path "build" -ErrorAction SilentlyContinue
          cd build
          cmake .. -G "Visual Studio 17 2022" -A x64 -DCMAKE_BUILD_TYPE=Release -DLLAMA_CURL=OFF -DLLAMA_NATIVE=ON
          cmake --build . --config Release
          cd ..
          $dst = Join-Path $env:RUNNER_TOOL_CACHE "llama.cpp_${{ inputs.llama_version }}\bin"
          mkdir $dst -Force | Out-Null
          Copy-Item .\build\bin\Release\*.exe $dst -Force
          Copy-Item .\build\bin\Release\*.dll $dst -Force

      - name: Download model
        shell: pwsh
        run: |
            $modelId = "${{ inputs.model }}"
            $dir = Join-Path $env:RUNNER_TOOL_CACHE "models"
            if (-not (Test-Path $dir)) { mkdir $dir }

            # 単一/複数ファイルに対応するため配列を使用（差分最小：switchの中身はURL/ファイル名の列挙のみ）
            $urls = @()
            $outs = @()

            switch ($modelId) {
              "gemma-3-4b-it-q4_0"   { $urls=@("https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/gemma-3-4b-it-q4_0.gguf");  $outs=@("gemma-3-4b-it-q4_0.gguf") }
              "gemma-3-12b-it-q4_0"  { $urls=@("https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/resolve/main/gemma-3-12b-it-q4_0.gguf"); $outs=@("gemma-3-12b-it-q4_0.gguf") }
              "gemma-3-27b-it-q4_0"  { $urls=@("https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf/resolve/main/gemma-3-27b-it-q4_0.gguf"); $outs=@("gemma-3-27b-it-q4_0.gguf") }

              "gemma-3-27b-it-qat-Q8_0"    { $urls=@("https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF/resolve/main/google_gemma-3-27b-it-qat-Q8_0.gguf");   $outs=@("google_gemma-3-27b-it-qat-Q8_0.gguf") }
              "gemma-3-27b-it-qat-IQ4_NL"  { $urls=@("https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF/resolve/main/google_gemma-3-27b-it-qat-IQ4_NL.gguf"); $outs=@("google_gemma-3-27b-it-qat-IQ4_NL.gguf") }
              "gemma-3-27b-it-qat-IQ3_XXS" { $urls=@("https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF/resolve/main/google_gemma-3-27b-it-qat-IQ3_XXS.gguf");$outs=@("google_gemma-3-27b-it-qat-IQ3_XXS.gguf") }
              "gemma-3-27b-it-qat-IQ3_M"   { $urls=@("https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF/resolve/main/google_gemma-3-27b-it-qat-IQ3_M.gguf");  $outs=@("google_gemma-3-27b-it-qat-IQ3_M.gguf") }
              "gemma-3-27b-it-IQ3_XXS"     { $urls=@("https://huggingface.co/bartowski/google_gemma-3-27b-it-GGUF/resolve/main/google_gemma-3-27b-it-IQ3_XXS.gguf");        $outs=@("google_gemma-3-27b-it-IQ3_XXS.gguf") }
              "gemma-3-27b-it-IQ3_M"       { $urls=@("https://huggingface.co/bartowski/google_gemma-3-27b-it-GGUF/resolve/main/google_gemma-3-27b-it-IQ3_M.gguf");          $outs=@("google_gemma-3-27b-it-IQ3_M.gguf") }

              "openai_gpt-oss-20b-Q8_0"    { $urls=@("https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-Q8_0.gguf");   $outs=@("openai_gpt-oss-20b-Q8_0.gguf") }
              "openai_gpt-oss-20b-IQ4_NL"  { $urls=@("https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-IQ4_NL.gguf"); $outs=@("openai_gpt-oss-20b-IQ4_NL.gguf") }
              "openai_gpt-oss-20b-MXFP4"   { $urls=@("https://huggingface.co/bartowski/openai_gpt-oss-20b-GGUF/resolve/main/openai_gpt-oss-20b-MXFP4.gguf");  $outs=@("openai_gpt-oss-20b-MXFP4.gguf") }

              "openai_gpt-oss-120b-Q4_K_M" {
                $urls = @(
                  "https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/Q4_K_M/gpt-oss-120b-Q4_K_M-00001-of-00002.gguf",
                  "https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/Q4_K_M/gpt-oss-120b-Q4_K_M-00002-of-00002.gguf"
                )
                $outs = @(
                  "gpt-oss-120b-Q4_K_M-00001-of-00002.gguf",
                  "gpt-oss-120b-Q4_K_M-00002-of-00002.gguf"
                )
              }

              "openai_gpt-oss-120b-IQ4_NL" {
                $urls = @(
                  "https://huggingface.co/bartowski/openai_gpt-oss-120b-GGUF/resolve/main/openai_gpt-oss-120b-IQ4_NL/openai_gpt-oss-120b-IQ4_NL-00001-of-00002.gguf",
                  "https://huggingface.co/bartowski/openai_gpt-oss-120b-GGUF/resolve/main/openai_gpt-oss-120b-IQ4_NL/openai_gpt-oss-120b-IQ4_NL-00002-of-00002.gguf"
                )
                $outs = @(
                  "openai_gpt-oss-120b-IQ4_NL-00001-of-00002.gguf",
                  "openai_gpt-oss-120b-IQ4_NL-00002-of-00002.gguf"
                )
              }
          
              "openai_gpt-oss-120b-UD-Q4_K_XL" {
                $urls = @(
                  "https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/UD-Q4_K_XL/gpt-oss-120b-UD-Q4_K_XL-00001-of-00002.gguf",
                  "https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/UD-Q4_K_XL/gpt-oss-120b-UD-Q4_K_XL-00002-of-00002.gguf"
                )
                $outs = @(
                  "gpt-oss-120b-UD-Q4_K_XL-00001-of-00002.gguf",
                  "gpt-oss-120b-UD-Q4_K_XL-00002-of-00002.gguf"
                )
              }

              "cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ3_XXS" { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ3_XXS.gguf"); $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ3_XXS.gguf") }
              "cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ3_M"   { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ3_M.gguf");   $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ3_M.gguf") }
              "cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ4_NL"  { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ4_NL.gguf");  $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-IQ4_NL.gguf") }
              "cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q3_K_M"  { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q3_K_M.gguf");  $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q3_K_M.gguf") }
              "cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_0"    { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_0.gguf");    $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_0.gguf") }
              "cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M"  { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf");  $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf") }

              "cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ3_XXS" { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ3_XXS.gguf"); $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ3_XXS.gguf") }
              "cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ3_M"   { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ3_M.gguf");   $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ3_M.gguf") }
              "cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ4_NL"  { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ4_NL.gguf");  $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-IQ4_NL.gguf") }
              "cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q4_0"    { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q4_0.gguf");    $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q4_0.gguf") }
              "cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q3_K_M"  { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q3_K_M.gguf");  $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q3_K_M.gguf") }
              "cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q4_K_M"  { $urls=@("https://huggingface.co/mmnga/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-gguf/resolve/main/cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q4_K_M.gguf");  $outs=@("cyberagent-DeepSeek-R1-Distill-Qwen-14B-Japanese-Q4_K_M.gguf") }

              "Qwen3-30B-A3B-Q3_K_M"      { $urls=@("https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q3_K_M.gguf");      $outs=@("Qwen3-30B-A3B-Q3_K_M.gguf") }
              "Qwen3-30B-A3B-UD-Q3_K_XL"  { $urls=@("https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-Q3_K_XL.gguf");  $outs=@("Qwen3-30B-A3B-UD-Q3_K_XL.gguf") }
              "Qwen3-30B-A3B-UD-IQ3_XXS"  { $urls=@("https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-UD-IQ3_XXS.gguf");  $outs=@("Qwen3-30B-A3B-UD-IQ3_XXS.gguf") }
              "Qwen3-30B-A3B-Q4_0"        { $urls=@("https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_0.gguf");        $outs=@("Qwen3-30B-A3B-Q4_0.gguf") }
              "Qwen3-30B-A3B-Q4_K_M"      { $urls=@("https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/resolve/main/Qwen3-30B-A3B-Q4_K_M.gguf");      $outs=@("Qwen3-30B-A3B-Q4_K_M.gguf") }

              "a-m-team_AM-Thinking-v1-IQ3_M" { $urls=@("https://huggingface.co/bartowski/a-m-team_AM-Thinking-v1-GGUF/resolve/main/a-m-team_AM-Thinking-v1-IQ3_M.gguf"); $outs=@("a-m-team_AM-Thinking-v1-IQ3_M.gguf") }

              default { throw "Unknown model: $modelId" }
            }

            for ($i = 0; $i -lt $urls.Count; $i++) {
              $path = Join-Path $dir $outs[$i]
              if (-not (Test-Path $path)) {
                # DL直前にHF認証ヘッダを必ず付与（元コードに近い位置）
                $hdr = @{ Authorization = "Bearer ${{ inputs.HUGGINGFACE_TOKEN }}" }
                try {
                  Invoke-WebRequest -Uri $urls[$i] -Headers $hdr -OutFile $path -ErrorAction Stop
                } catch {
                  Write-Error "Download failed: $($urls[$i])"
                  exit 1
                }
              }
              if ($i -eq 0) {
                "MODEL_PATH=$path" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
              }
            }


      - name: Set llama.dll path
        if: ${{ steps.gpu_check.outputs.nvidia_gpu != 'True' }}
        shell: pwsh
        run: |
          $dll = Join-Path $env:RUNNER_TOOL_CACHE "llama.cpp_${{ inputs.llama_version }}\bin"
          "LLAMA_CPP_LIB_PATH=$dll" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append

      - name: Set llama.dll path (nvidia)
        if: ${{ steps.gpu_check.outputs.nvidia_gpu == 'True' }}
        shell: pwsh
        run: |
          $dll = Join-Path $env:RUNNER_TOOL_CACHE "llama_cuda.cpp_${{ inputs.llama_version }}\bin"
          "LLAMA_CPP_LIB_PATH=$dll" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append

      - id: generate_uuid
        run: |
          $uuid = [guid]::NewGuid().ToString()
          "uuid=$uuid" >> $env:GITHUB_OUTPUT
        shell: pwsh

      - name: Write question to file
        if: ${{ inputs.question != '' && inputs.question != null }}
        shell: pwsh
        run: |
            chcp 65001
            $promptFile = "${{ runner.temp }}\llm_prompt_${{ steps.generate_uuid.outputs.uuid }}.txt"
            $question = @"
            ${{ inputs.question }}
            "@
            $question | Out-File -Encoding UTF8 $promptFile

      - name: Copy question file to temp
        if: ${{ inputs.question_file != '' && inputs.question_file != null }}
        shell: pwsh
        run: |
            $promptFile = "${{ runner.temp }}\llm_prompt_${{ steps.generate_uuid.outputs.uuid }}.txt"
            Copy-Item -Path "${{ inputs.question_file }}" -Destination $promptFile -Force
            
      - name: Run llama with model ${{ inputs.model }} (gpu=${{ steps.gpu_check.outputs.nvidia_gpu }})
        id: inference
        shell: pwsh
        run: |
          chcp 65001

          $exe = Join-Path $env:LLAMA_CPP_LIB_PATH "llama-cli.exe"
          $model = $env:MODEL_PATH
          $promptFile = "${{ runner.temp }}\llm_prompt_${{ steps.generate_uuid.outputs.uuid }}.txt"
          $outputFile = "${{ runner.temp }}\llm_output.txt"

          "exe=${exe}" >> $env:GITHUB_OUTPUT
          
          if (-not (Test-Path $promptFile)) {
              Write-Host "Could not find the prompt file. Inference will be skipped."
              exit 0
          }

          $cmd = "& `"$exe`" --model `"$model`" --file `"$promptFile`" --no-display-prompt --keep 0 --batch-size ${{ inputs.batch-size }}"

          # GPUオプションの条件付き付与
          if ("${{ steps.gpu_check.outputs.nvidia_gpu }}" -eq "True") {
            $cmd += " --n-gpu-layers ${{ inputs.n-gpu-layers }}"
          }
          if ("${{ inputs.seed }}" -ne "0") {
            $cmd += " --seed ${{ inputs.seed }}"
          }
          if ("${{ inputs.threads }}" -ne "0") {
            $cmd += " --threads ${{ inputs.threads }}"
          }
          if ("${{ inputs.mlock }}" -eq "true") {
            $cmd += " --mlock"
          }
          if ("${{ inputs.simple-io }}" -eq "true") {
            $cmd += " --simple-io"
          }

          $cmd += " --mirostat 2 -c ${{ inputs.ctx-size }} -no-cnv -n ${{ inputs.predict }}"

          Write-Host "Executing command:"
          Write-Host $cmd

          Invoke-Expression "$cmd > `"$outputFile`""

      - name: Set outputs
        run: |
            chcp 65001
            $tempPath = Join-Path ${{ runner.temp }} "llm_output.txt"
            if (-not (Test-Path -Path $tempPath)) {
              Write-Host "🚫Skipped. Log file does not exist at: $tempPath"
              exit 0
            }
            $answer = Get-Content $tempPath -Raw
            $delimiter = -join ((1..10) | %{(65..90) + (97..122) + (48..57) | Get-Random} | % {[char]$_})
            Write-Output "answer<<${delimiter}" >> $env:GITHUB_OUTPUT
            Write-Output ${answer} >> $env:GITHUB_OUTPUT
            Write-Output "${delimiter}" >> $env:GITHUB_OUTPUT
        shell: pwsh
        id: out